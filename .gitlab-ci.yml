stages:
 - lint
 - build
 - test
 - cleanup

variables:
  # Variables required by Common CI jobs
  CI_COMMON_JOB_VERSION: "18b2a600936c6f7945d6f1131f606c7083a515ff"
  DOCKER_BUILDER_TAG: "$CI_COMMON_JOB_VERSION"
  DOCKER_DIND_TAG: "$CI_COMMON_JOB_VERSION"
  IMAGE_REMOVER_TAG: "$CI_COMMON_JOB_VERSION"
  # Git configuration
  GIT_STRATEGY: clone
  GIT_SUBMODULE_STRATEGY: recursive
  GIT_SUBMODULE_DEPTH: 1
  GIT_SUBMODULE_UPDATE_FLAGS: --jobs 4
  # HAF configuration
  DATA_CACHE_HAF_PREFIX: "/cache/replay_data_haf_for_hafbe_"
  BLOCK_LOG_SOURCE_DIR_5M: /blockchain/block_log_5m

include:
  - template: Workflows/Branch-Pipelines.gitlab-ci.yml
  - project: hive/haf
    ref: fc8a83864e162fd5a01f462127da31cef7293cea # develop
    file: /scripts/ci-helpers/prepare_data_image_job.yml # implicitly pulls templates/base.gitlab-ci.yml from common-ci-configuration

.lint_job:
  extends: .job-defaults
  stage: lint
  variables:
    GIT_SUBMODULE_STRATEGY: none
  artifacts:
    name: lint-results
    when: always
  tags:
    - public-runner-docker

lint_bash_scripts:
  extends: .lint_job
  image: koalaman/shellcheck-alpine:latest
  before_script:
    - apk add xmlstarlet
  script:
    - find . -name .git -type d -prune -o -type f -name \*.sh -exec shellcheck -f checkstyle {} + | tee shellcheck-checkstyle-result.xml
  after_script:
    - xmlstarlet tr misc/checkstyle2junit.xslt shellcheck-checkstyle-result.xml > shellcheck-junit-result.xml
  artifacts:
    paths: 
      - shellcheck-checkstyle-result.xml
      - shellcheck-junit-result.xml
    reports:
      junit: shellcheck-junit-result.xml

lint_sql_scripts:
  extends: .lint_job
  image: 
    name: sqlfluff/sqlfluff:2.1.4
    entrypoint: [""]
  script:
    - sqlfluff lint --format yaml --write-output sql-lint.yaml
  artifacts:
    paths:
      - sql-lint.yaml

prepare_haf_image:
  stage: build
  extends: .prepare_haf_image
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/submodules/haf"
    REGISTRY_USER: "$HAF_DEPLOY_USERNAME"
    REGISTRY_PASS: "$HAF_DEPLOY_TOKEN"
  tags:
    - public-runner-docker
    - hived-for-tests

prepare_haf_data:
  extends: .prepare_haf_data_5m
  needs:
    - job: prepare_haf_image
      artifacts: true
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/submodules/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/submodules/haf/docker/config_5M.ini"
  tags:
    - data-cache-storage

.docker-base-build-template:
  extends: .docker_image_builder_job_template
  stage: build
  variables:
    BASE_REPO_NAME: ""
    TAG: ""
    NAME: ""
    TARGET: "$NAME"
    PROGRESS_DISPLAY: "plain"
  before_script:
    - !reference [.docker_image_builder_job_template, before_script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
  script:
    - |
      echo -e "\e[0Ksection_end:$(date +%s):tag\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):build[collapsed=true]\r\e[0KBaking $NAME${BASE_REPO_NAME:+/$BASE_REPO_NAME} image..."
      function image-exists() {
        local image=$1
        docker manifest inspect "$1" > /dev/null
        return $?
      }
      if image-exists "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${TAG}"; then
        echo "Image $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${TAG} already exists. Skipping..."
        if [[ -n "$CI_COMMIT_TAG" && "$TARGET" == "full-ci" ]]; then
          echo "Tagging pre-existing image with Git tag..."
          docker pull "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${TAG}"
          docker tag "$CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${TAG}" "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
          docker push "${CI_REGISTRY_IMAGE}:${CI_COMMIT_TAG}"
        fi
      else
        echo "Baking $CI_REGISTRY_IMAGE${NAME:+/$NAME}${BASE_REPO_NAME:+/$BASE_REPO_NAME}:${TAG} image..."
        git config --global --add safe.directory $(pwd)
        scripts/build_instance.sh "$CI_PROJECT_DIR"
      fi
      echo -e "\e[0Ksection_end:$(date +%s):build\r\e[0K"
  tags:
    - public-runner-docker

docker-ci-runner-build:
  extends: .docker-base-build-template
  variables:
    BASE_REPO_NAME: ""
    TAG: "docker-24.0.1-3"
    NAME: "ci-runner"
    TARGET: "ci-runner-ci"

docker-psql-client-build:
  extends: .docker-base-build-template
  variables:
    BASE_REPO_NAME: ""
    TAG: "14"
    NAME: "psql"
    TARGET: "psql-ci"

docker-setup-docker-image-build:
  extends: .docker-base-build-template
  variables:
    GIT_SUBMODULE_STRATEGY: none
    GIT_DEPTH: 1
    BASE_REPO_NAME: ""
    TAG: "$CI_COMMIT_SHORT_SHA"
    NAME: ""
    TARGET: "full-ci"

.test-template:
  extends: .docker_image_builder_job_template
  stage: test
  image: registry.gitlab.syncad.com/hive/haf_block_explorer/ci-runner:docker-24.0.1-3
  variables:
    COMPOSE_OPTIONS_STRING: "" # to be overrided by derived jobs

    HAF_COMMAND: --shared-file-size=1G --plugin database_api --replay --stop-replay-at-block=5000000

    BASE_DIRECTORY: ${CI_PROJECT_DIR}/docker_${CI_JOB_NAME}
    HAF_DATA_DIRECTORY: ${BASE_DIRECTORY}/datadir
    HAF_SHM_DIRECTORY: ${BASE_DIRECTORY}/shm_dir

    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMMAND: SELECT CASE WHEN irreversible_block = 5000000 THEN 0 ELSE 1 END FROM hive.contexts WHERE name = 'hafbe_app';
    MESSAGE: Waiting for HAF Block Explorer to finish processing blocks...

    FF_ENABLE_JOB_CLEANUP: 1
    FF_NETWORK_PER_BUILD: 1
    PROGRESS_DISPLAY: "plain"

  timeout: 2 hours
  before_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
      git config --global --add safe.directory "$CI_PROJECT_DIR"
      git config --global --add safe.directory "$CI_PROJECT_DIR/submodules/haf"
      echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  script:
    - |
      set -x
      echo -e "\e[0Ksection_start:$(date +%s):prepare_data[collapsed=true]\r\e[0KPreparing HAF data..."

      echo "$(whoami) ALL=(ALL) NOPASSWD:ALL" > ./sudoers
      sudo cp -vf ./sudoers /etc/sudoers

      cp -vfr "${CI_PROJECT_DIR}/docker" "${BASE_DIRECTORY}"

      sudo cp -vfr -t "${BASE_DIRECTORY}" "${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}/datadir"
      sudo cp -vfr -t "${BASE_DIRECTORY}" "${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}/shm_dir"

      sudo chmod -Rc a+w "${BASE_DIRECTORY}/datadir/blockchain"
      sudo chmod -Rc a+w "${BASE_DIRECTORY}/datadir/haf_postgresql_conf.d"
      sudo chmod -Rc a+w "${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}/shm_dir"

      sudo rm -fv "${BASE_DIRECTORY}/datadir/*.log"

      echo -e "\e[0Ksection_end:$(date +%s):prepare_data\r\e[0K"

      echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=false]\r\e[0KStarting the test environment..."

      echo "Entering a base directory ${BASE_DIRECTORY}"

      cd "${BASE_DIRECTORY}"

      # Put all the variables that can be predefined in the "variables"
      # block and all the variables that have to be dynamically set by the script below
      {
        echo "HAF_REGISTRY=$HAF_REGISTRY_PATH"
        echo "HAF_VERSION=$HAF_REGISTRY_TAG"
        echo "HIVED_UID=$HIVED_UID"
        echo "SETUP_UID=$(id -u)"
        echo "HAF_DATA_DIRECTORY=${HAF_DATA_DIRECTORY}"
        echo "HAF_SHM_DIRECTORY=${HAF_SHM_DIRECTORY}"

      } > .env.local
      cat .env.local
      echo "Docker Compose options string: $COMPOSE_OPTIONS_STRING"
      IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
      echo "Docker Compose options: ${COMPOSE_OPTIONS[@]}"
      docker compose version
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" config | tee docker-compose-config.yml.log
      timeout -s INT -k 60 600 docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" up --remove-orphans -V --force-recreate --wait --quiet-pull
      #--detach

      echo "Returning to project directory ${CI_PROJECT_DIR}"

      cd "${CI_PROJECT_DIR}"

      echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0K$MESSAGE"

      function wait-for-haf-be-startup() {
        until psql "$POSTGRES_ACCESS" --quiet --tuples-only --command="$COMMAND" | grep 0 &>/dev/null
        do 
          echo "$MESSAGE"
          sleep 3
        done
      }
      export -f wait-for-haf-be-startup
      export POSTGRES_ACCESS
      export COMMAND
      export MESSAGE

      timeout -k 60 600 bash -c wait-for-haf-be-startup
      echo "Block processing is finished."

      echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"

  after_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):stop_env[collapsed=true]\r\e[0KStopping test environment..."

      pushd "${BASE_DIRECTORY}"
      IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" ps -a
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" logs haf > haf.log
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" logs backend-setup > backend-setup.log
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > backend-block-processing.log
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > backend-postgrest.log
      docker compose --project-name hafbe --project-directory "${BASE_DIRECTORY}" --env-file .env.local --ansi never "${COMPOSE_OPTIONS[@]}" down --volumes
      popd

      tar -cf - ${BASE_DIRECTORY}/*.log ${BASE_DIRECTORY}/datadir/*.log | 7z a -si -mx9 "${CI_JOB_NAME}-container-logs.tar.7z"

      echo -e "\e[0Ksection_end:$(date +%s):stop_env\r\e[0K"

  artifacts:
    paths:
      - "*-container-logs.tar.7z"

    expire_in: 1 week
    when: always
  tags:
    - data-cache-storage

.psql-client-test-template:
  extends: .test-template
  needs:
    - prepare_haf_image
    - prepare_haf_data
    - docker-psql-client-build
    - docker-ci-runner-build
  variables:
    PGHERO_USERNAME: unused
    PGHERO_PASSWORD: unused
    COMPOSE_OPTIONS_STRING: --file docker-compose.yml --file ci/docker-compose.yml

.full-image-test-template:
  extends: .test-template
  needs:
    - prepare_haf_image
    - prepare_haf_data
    - docker-setup-docker-image-build
    - docker-ci-runner-build
  variables:
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    COMPOSE_OPTIONS_STRING: --file ci/docker-compose.full.yml

psql-client-regression-test:
  extends: .psql-client-test-template

  script:
    - !reference [.test-template, script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tests[collapsed=true]\r\e[0KRunning tests..."

      cd tests/account_parameters
      ./accounts_dump_test.sh --host=docker

      echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"

.psql-client-performance-test:
  extends: .psql-client-test-template
  script:
    - !reference [.test-template, script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tests[collapsed=true]\r\e[0KRunning tests..."

      ./tests/run_performance_tests.sh --postgresql-host=docker --postgrest-host=docker --database-size=6000 --test-loop-count=1000
      tar -cf - $(pwd)/tests/performance/result* | 7z a -si -mx9 tests/performance/results.tar.7z
      cat jmeter.log | python3 docker/ci/parse-jmeter-output.py
      m2u --input $(pwd)/tests/performance/result/result.xml --output $(pwd)/tests/performance/junit-result.xml

      echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
      - "*-container-logs.tar.7z"
      - tests/performance/result/result_report/
      - tests/performance/results.tar.7z
      - jmeter.log
    reports:
      junit: tests/performance/junit-result.xml

.full-image-regression-test:
  extends: .full-image-test-template
  script:
    - !reference [psql-client-regression-test, script]
  artifacts:
    paths:
      !reference [psql-client-regression-test, artifacts, paths]

.full-image-performance-test:
  extends: .full-image-test-template
  script:
    - !reference [.psql-client-performance-test, script]
  artifacts:
    paths:
      !reference [.psql-client-performance-test, artifacts, paths]
    reports:
      junit: !reference [.psql-client-performance-test, artifacts, reports, junit]

cleanup_obsolete_haf_cache_manual:
  extends: .cleanup_cache_manual
  stage: cleanup
  variables:
    GIT_STRATEGY: none
    CLEANUP_PATH_PATTERN: "/cache/replay_data_haf_block_explorer_haf_*"

  tags:
    - data-cache-storage

cleanup_haf_cache_manual:
  extends: .cleanup_cache_manual
  stage: cleanup
  variables:
    GIT_STRATEGY: none
    CLEANUP_PATH_PATTERN: "${DATA_CACHE_HAF_PREFIX}_*"

  tags:
    - data-cache-storage
