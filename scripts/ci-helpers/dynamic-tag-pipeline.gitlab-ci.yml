stages:
 - lint
 - build
 - test
 - cleanup

include:
  - local: /scripts/ci-helpers/includes.gitlab-ci.yml

############ Templates ############

.test-template:
  extends: 
    - .docker_image_builder_job_template
    - .dynamic-runner-job-template
  stage: test
  image: registry.gitlab.syncad.com/hive/haf_block_explorer/ci-runner:docker-24.0.1-4
  variables:
    HAF_COMMAND: --shared-file-size=1G --plugin database_api --replay --stop-at-block=5000000
    DATA_SOURCE: ${DATA_CACHE_HAF_PREFIX}_${HAF_COMMIT}
    DATADIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/datadir
    SHM_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/shm_dir
    HAF_DATA_DIRECTORY: ${DATADIR}
    HAF_SHM_DIRECTORY: ${SHM_DIR}
    CONTAINER_LOG_DIR: ${CI_PROJECT_DIR}/${CI_JOB_ID}/logs
    POSTGRES_ACCESS: postgresql://haf_admin@docker:5432/haf_block_log
    COMMAND: SELECT CASE WHEN irreversible_block = 5000000 THEN 0 ELSE 1 END FROM hive.contexts WHERE name = 'hafbe_app';
    MESSAGE: Waiting for HAF Block Explorer to finish processing blocks...
  timeout: 1 hours
  before_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):login[collapsed=true]\r\e[0KLogging to Docker registry..."
      docker login -u "$CI_REGISTRY_USER" -p "$CI_REGISTRY_PASSWORD" $CI_REGISTRY
      echo -e "\e[0Ksection_end:$(date +%s):login\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):git[collapsed=true]\r\e[0KConfiguring Git..."
      git config --global --add safe.directory "$CI_PROJECT_DIR"
      git config --global --add safe.directory "$CI_PROJECT_DIR/submodules/haf"
      echo -e "\e[0Ksection_end:$(date +%s):git\r\e[0K"
  script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):compose[collapsed=true]\r\e[0KStarting the test environment..."

      cp /blockchain/block_log_5m/block_log docker/blockchain/block_log
      chmod a+w docker/blockchain/block_log

      "${CI_PROJECT_DIR}/submodules/haf/scripts/copy_datadir.sh"

      pushd docker
      # Put all the variables that can be predefined in the "variables"
      # block and all the variables that have to be dynamically set by the script below
      {
        echo "HAF_REGISTRY=$HAF_REGISTRY_PATH"
        echo "HAF_VERSION=$HAF_REGISTRY_TAG"
        echo "HIVED_UID=$HIVED_UID"
        echo "SETUP_UID=$(id -u)"
      } > .env.local
      cat .env.local
      echo "Docker Compose options string: $COMPOSE_OPTIONS_STRING"
      IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
      echo "Docker Compose options: ${COMPOSE_OPTIONS[@]}"
      mkdir -p "$CONTAINER_LOG_DIR"
      docker compose config | tee "${CONTAINER_LOG_DIR}/docker-compose-config.yml.log"
      timeout -s INT -k 1m 20m docker compose ${COMPOSE_OPTIONS[@]} up --detach
      popd

      echo -e "\e[0Ksection_end:$(date +%s):compose\r\e[0K"
      echo -e "\e[0Ksection_start:$(date +%s):wait[collapsed=true]\r\e[0K$MESSAGE"

      function wait-for-haf-be-startup() {
        until psql "$POSTGRES_ACCESS" --quiet --tuples-only --command="$COMMAND" | grep 0 &>/dev/null
        do 
          echo "$MESSAGE"
          sleep 3
        done
      }
      export -f wait-for-haf-be-startup
      export POSTGRES_ACCESS
      export COMMAND
      export MESSAGE

      timeout -k 1m 10m bash -c wait-for-haf-be-startup
      
      echo "Block processing is finished."

      echo -e "\e[0Ksection_end:$(date +%s):wait\r\e[0K"
  artifacts:
    expire_in: 1 week
    when: always

.psql-client-test-template:
  extends: .test-template
  needs:
    - prepare_haf_data
  variables:
    PGHERO_USERNAME: unused
    PGHERO_PASSWORD: unused
    COMPOSE_OPTIONS_STRING: --env-file .env.local --file docker-compose.yml --file ci/docker-compose.yml --ansi never
  after_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

      pushd docker
      IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
      docker compose "${COMPOSE_OPTIONS[@]}" logs haf > "${CONTAINER_LOG_DIR}/haf.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-setup > "${CONTAINER_LOG_DIR}/backend-setup.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > "${CONTAINER_LOG_DIR}/backend-block-processing.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > "${CONTAINER_LOG_DIR}/backend-postgrest.log"
      docker compose "${COMPOSE_OPTIONS[@]}" down --volumes
      popd

      tar -cf - "${CONTAINER_LOG_DIR}/*.log" | 7z a -si -mx9 "${CONTAINER_LOG_DIR}/container-logs.tar.gz"

      Manually remove the copy of the replay data to preserve disk space on the replay server
      sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

      echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"

.full-image-test-template:
  extends: .test-template
  needs:
    - prepare_haf_data
  variables:
    BACKEND_VERSION: "$CI_COMMIT_SHORT_SHA"
    COMPOSE_OPTIONS_STRING: --env-file .env.local --file ci/docker-compose.full.yml --ansi never
  after_script:
    - |
      echo -e "\e[0Ksection_start:$(date +%s):compose2[collapsed=true]\r\e[0KStopping test environment..."

      pushd docker
      IFS=" " read -ra COMPOSE_OPTIONS <<< $COMPOSE_OPTIONS_STRING
      docker compose "${COMPOSE_OPTIONS[@]}" logs haf > "${CONTAINER_LOG_DIR}/haf.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs app-setup > "${CONTAINER_LOG_DIR}/app-setup.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-setup > "${CONTAINER_LOG_DIR}/backend-setup.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-block-processing > "${CONTAINER_LOG_DIR}/backend-block-processing.log"
      docker compose "${COMPOSE_OPTIONS[@]}" logs backend-postgrest > "${CONTAINER_LOG_DIR}/backend-postgrest.log"
      docker compose "${COMPOSE_OPTIONS[@]}" down --volumes
      popd

      tar -cf - "${CONTAINER_LOG_DIR}/*.log" | 7z a -si -mx9 "${CONTAINER_LOG_DIR}/container-logs.tar.gz"

      Manually remove the copy of the replay data to preserve disk space on the replay server
      sudo rm -rf ${CI_PROJECT_DIR}/${CI_JOB_ID}

      echo -e "\e[0Ksection_end:$(date +%s):compose2\r\e[0K"

############ End templates ############

############ Build ############

prepare_haf_data:
  extends: 
    - .prepare_haf_data_5m
    - .dynamic-runner-job-template
  stage: build
  variables:
    SUBMODULE_DIR: "$CI_PROJECT_DIR/submodules/haf"
    BLOCK_LOG_SOURCE_DIR: $BLOCK_LOG_SOURCE_DIR_5M
    CONFIG_INI_SOURCE: "$CI_PROJECT_DIR/submodules/haf/docker/config_5M.ini"

############ End build ############

############ Test ############

psql-client-regression-test:
  extends: .psql-client-test-template
  script:
    - !reference [.test-template, script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tests[collapsed=true]\r\e[0KRunning tests..."

      cd tests/account_parameters
      ./accounts_dump_test.sh --host=docker

      echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
      - "${CONTAINER_LOG_DIR}/container-logs.tar.gz"
    expire_in: 1 week
    when: always

psql-client-performance-test:
  extends: .psql-client-test-template
  script:
    - !reference [.test-template, script]
    - |
      echo -e "\e[0Ksection_start:$(date +%s):tests[collapsed=true]\r\e[0KRunning tests..."

      timeout -k 1m 15m ./tests/run_performance_tests.sh --postgresql-host=docker --postgrest-host=docker --database-size=6000 --test-loop-count=1000
      tar -cf - $(pwd)/tests/performance/result* | 7z a -si -mx9 tests/performance/results.tar.7z
      cat jmeter.log | python3 docker/ci/parse-jmeter-output.py
      m2u --input $(pwd)/tests/performance/result/result.xml --output $(pwd)/tests/performance/result-junit.xml

      echo -e "\e[0Ksection_end:$(date +%s):tests\r\e[0K"
  artifacts:
    paths:
      - "${CONTAINER_LOG_DIR}/container-logs.tar.gz"
      - "tests/performance/result/result_report/"
      - "tests/performance/results.tar.7z"
      - "jmeter.log"
      - "tests/performance/result-junit.xml"
    reports:
      junit: tests/performance/result-junit.xml

full-image-regression-test:
  extends: .full-image-test-template
  script:
    - !reference [psql-client-regression-test, script]
  artifacts:
    paths:
      !reference [psql-client-regression-test, artifacts, paths]

full-image-performance-test:
  extends: .full-image-test-template
  script:
    - !reference [psql-client-performance-test, script]
  artifacts:
    paths:
      !reference [psql-client-performance-test, artifacts, paths]
    reports:
      junit: !reference [psql-client-performance-test, artifacts, reports, junit]

############ End test ############

############ Cleanup ############

cleanup_haf_cache_manual:
  extends: 
    - .cleanup_cache_manual_template
    - .dynamic-runner-job-template
  stage: cleanup
  variables:
    CLEANUP_PATH_PATTERN: "${DATA_CACHE_HAF_PREFIX}_*"

############ End cleanup ############
